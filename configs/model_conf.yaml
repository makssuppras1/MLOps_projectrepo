# Model architecture hyperparameters
# DistilBERT with optimized training strategy
model:
  num_labels: 5  # Reduced to 5 categories for ultra-fast training (top 4 + OTHER)
  model_name: distilbert-base-uncased  # DistilBERT: good balance of speed and accuracy
  dropout: 0.1  # Add dropout for regularization
  freeze_encoder: true  # Start frozen - will be unfrozen gradually during training
