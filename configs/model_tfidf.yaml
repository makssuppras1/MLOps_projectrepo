# TF-IDF + XGBoost model configuration
# Optimized for accuracy while remaining fast
# Memory-optimized: Reduced features/ngrams to fit in 32GB RAM (Vertex AI)
#
# ⚠️ LOCAL DOCKER REQUIREMENTS:
# - Full dataset (82k samples × 10k features) requires ~5-6GB RAM minimum
# - Ensure Docker Desktop memory is set to 8GB+ (Settings → Resources → Memory)
# - If OOM errors (ExitCode=137), reduce training.max_samples or model.max_features
# - See docs/OOM_FIX.md for detailed troubleshooting
model:
  num_labels: 5
  max_features: 10000  # Optimized from sweep test
  stop_words: "english"
  ngram_range_min: 1  # Minimum n-gram size
  ngram_range_max: 3  # Optimized from sweep test (unigrams + bigrams + trigrams)
  min_df: 5  # Optimized from sweep test
  max_df: 0.7152124944236803  # Optimized from sweep test
  max_depth: 8  # Optimized from sweep test
  learning_rate: 0.04297016347000527  # Optimized from sweep test
  n_estimators: 1000  # Optimized from sweep test
  subsample: 0.7654053111795991  # Optimized from sweep test
  colsample_bytree: 0.7066637762929179  # Optimized from sweep test
  reg_alpha: 0.03872366298547403  # Optimized from sweep test
  reg_lambda: 0.7404807901247237  # Optimized from sweep test
  early_stopping_rounds: 10  # Optimized from sweep test
  random_state: 42  # Random seed for reproducibility (should match training.seed)
